{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 67553,
          "databundleVersionId": 7504710,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30635,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyojit02/classification_fashion_images_RESNET_MODEl/blob/main/notebook1d80e7cf7a_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        #print(os.path.join(dirname, filename))\n",
        "        continue\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-01-26T13:31:31.435836Z",
          "iopub.execute_input": "2024-01-26T13:31:31.436129Z",
          "iopub.status.idle": "2024-01-26T13:31:53.142318Z",
          "shell.execute_reply.started": "2024-01-26T13:31:31.436104Z",
          "shell.execute_reply": "2024-01-26T13:31:53.141250Z"
        },
        "trusted": true,
        "id": "m4QvcAHeUq-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pywt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def w2d(img, mode='haar', level=2):\n",
        "    imArray = img\n",
        "    # Datatype conversions\n",
        "    # Convert to grayscale\n",
        "    imArray = cv2.cvtColor(imArray, cv2.COLOR_RGB2GRAY)\n",
        "    # Convert to float\n",
        "    imArray = np.float32(imArray)\n",
        "    imArray /= 255\n",
        "    # Compute coefficients\n",
        "    coeffs = pywt.wavedec2(imArray, mode, level=level)\n",
        "\n",
        "    # Process Coefficients\n",
        "    coeffs_H = list(coeffs)\n",
        "    coeffs_H[0] *= 0\n",
        "\n",
        "    # Reconstruction\n",
        "    imArray_H = pywt.waverec2(coeffs_H, mode)\n",
        "    imArray_H *= 255\n",
        "    imArray_H = np.uint8(imArray_H)\n",
        "\n",
        "    return imArray_H"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T13:31:53.144393Z",
          "iopub.execute_input": "2024-01-26T13:31:53.144788Z",
          "iopub.status.idle": "2024-01-26T13:32:05.405814Z",
          "shell.execute_reply.started": "2024-01-26T13:31:53.144762Z",
          "shell.execute_reply": "2024-01-26T13:32:05.404744Z"
        },
        "trusted": true,
        "id": "a4PhwEH6Uq-W",
        "outputId": "2c325505-9ad5-49e9-f264-762329919dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to load data from a given directory\n",
        "def load_data(directory_path, csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    X = []  # List to store images\n",
        "    y = []  # List to store labels\n",
        "\n",
        "    if not os.path.exists(directory_path):\n",
        "        raise ValueError(f\"The specified directory '{directory_path}' does not exist.\")\n",
        "\n",
        "    for i in range(df.shape[0]):\n",
        "        img_path = os.path.join(directory_path, df.iloc[i]['file_name'])\n",
        "        label = df.iloc[i]['label']\n",
        "\n",
        "        # Read the image using cv2.imread\n",
        "        img = cv2.imread(img_path)\n",
        "        # Resize the image\n",
        "        img_resized = cv2.resize(img, (150, 150))\n",
        "        # Perform wavelet transform\n",
        "       # img_wvlet = w2d(img_resized, mode='db2', level=5)\n",
        "\n",
        "        # Stack the single-channel image to create a three-channel image\n",
        "      #  img_wvlet_rgb = np.stack([img_wvlet, img_wvlet, img_wvlet], axis=-1)\n",
        "\n",
        "       # img_wvlet_rgb = img_wvlet_rgb / 255\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(i, img_resized.shape)\n",
        "\n",
        "        X.append(img_resized)\n",
        "        y.append(label)\n",
        "\n",
        "    if not X or not y:\n",
        "        raise ValueError(f\"No images found in the specified directory '{directory_path}'.\")\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "data_directory = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/train'\n",
        "csv_path = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/train.csv'\n",
        "X, y = load_data(data_directory, csv_path)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T13:32:05.406890Z",
          "iopub.execute_input": "2024-01-26T13:32:05.407471Z",
          "iopub.status.idle": "2024-01-26T13:34:42.601502Z",
          "shell.execute_reply.started": "2024-01-26T13:32:05.407438Z",
          "shell.execute_reply": "2024-01-26T13:34:42.600503Z"
        },
        "trusted": true,
        "id": "bDwKGCEtUq-Y",
        "outputId": "b213f303-f1fc-4f7b-ff55-2cb6eb2a3cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "0 (150, 150, 3)\n100 (150, 150, 3)\n200 (150, 150, 3)\n300 (150, 150, 3)\n400 (150, 150, 3)\n500 (150, 150, 3)\n600 (150, 150, 3)\n700 (150, 150, 3)\n800 (150, 150, 3)\n900 (150, 150, 3)\n1000 (150, 150, 3)\n1100 (150, 150, 3)\n1200 (150, 150, 3)\n1300 (150, 150, 3)\n1400 (150, 150, 3)\n1500 (150, 150, 3)\n1600 (150, 150, 3)\n1700 (150, 150, 3)\n1800 (150, 150, 3)\n1900 (150, 150, 3)\n2000 (150, 150, 3)\n2100 (150, 150, 3)\n2200 (150, 150, 3)\n2300 (150, 150, 3)\n2400 (150, 150, 3)\n2500 (150, 150, 3)\n2600 (150, 150, 3)\n2700 (150, 150, 3)\n2800 (150, 150, 3)\n2900 (150, 150, 3)\n3000 (150, 150, 3)\n3100 (150, 150, 3)\n3200 (150, 150, 3)\n3300 (150, 150, 3)\n3400 (150, 150, 3)\n3500 (150, 150, 3)\n3600 (150, 150, 3)\n3700 (150, 150, 3)\n3800 (150, 150, 3)\n3900 (150, 150, 3)\n4000 (150, 150, 3)\n4100 (150, 150, 3)\n4200 (150, 150, 3)\n4300 (150, 150, 3)\n4400 (150, 150, 3)\n4500 (150, 150, 3)\n4600 (150, 150, 3)\n4700 (150, 150, 3)\n4800 (150, 150, 3)\n4900 (150, 150, 3)\n5000 (150, 150, 3)\n5100 (150, 150, 3)\n5200 (150, 150, 3)\n5300 (150, 150, 3)\n5400 (150, 150, 3)\n5500 (150, 150, 3)\n5600 (150, 150, 3)\n5700 (150, 150, 3)\n5800 (150, 150, 3)\n5900 (150, 150, 3)\n6000 (150, 150, 3)\n6100 (150, 150, 3)\n6200 (150, 150, 3)\n6300 (150, 150, 3)\n6400 (150, 150, 3)\n6500 (150, 150, 3)\n6600 (150, 150, 3)\n6700 (150, 150, 3)\n6800 (150, 150, 3)\n6900 (150, 150, 3)\n7000 (150, 150, 3)\n7100 (150, 150, 3)\n7200 (150, 150, 3)\n7300 (150, 150, 3)\n7400 (150, 150, 3)\n7500 (150, 150, 3)\n7600 (150, 150, 3)\n7700 (150, 150, 3)\n7800 (150, 150, 3)\n7900 (150, 150, 3)\n8000 (150, 150, 3)\n8100 (150, 150, 3)\n8200 (150, 150, 3)\n8300 (150, 150, 3)\n8400 (150, 150, 3)\n8500 (150, 150, 3)\n8600 (150, 150, 3)\n8700 (150, 150, 3)\n8800 (150, 150, 3)\n8900 (150, 150, 3)\n9000 (150, 150, 3)\n9100 (150, 150, 3)\n9200 (150, 150, 3)\n9300 (150, 150, 3)\n9400 (150, 150, 3)\n9500 (150, 150, 3)\n9600 (150, 150, 3)\n9700 (150, 150, 3)\n9800 (150, 150, 3)\n9900 (150, 150, 3)\n10000 (150, 150, 3)\n10100 (150, 150, 3)\n10200 (150, 150, 3)\n10300 (150, 150, 3)\n10400 (150, 150, 3)\n10500 (150, 150, 3)\n10600 (150, 150, 3)\n10700 (150, 150, 3)\n10800 (150, 150, 3)\n10900 (150, 150, 3)\n11000 (150, 150, 3)\n11100 (150, 150, 3)\n11200 (150, 150, 3)\n11300 (150, 150, 3)\n11400 (150, 150, 3)\n11500 (150, 150, 3)\n11600 (150, 150, 3)\n11700 (150, 150, 3)\n11800 (150, 150, 3)\n11900 (150, 150, 3)\n12000 (150, 150, 3)\n12100 (150, 150, 3)\n12200 (150, 150, 3)\n12300 (150, 150, 3)\n12400 (150, 150, 3)\n12500 (150, 150, 3)\n12600 (150, 150, 3)\n12700 (150, 150, 3)\n12800 (150, 150, 3)\n12900 (150, 150, 3)\n13000 (150, 150, 3)\n13100 (150, 150, 3)\n13200 (150, 150, 3)\n13300 (150, 150, 3)\n13400 (150, 150, 3)\n13500 (150, 150, 3)\n13600 (150, 150, 3)\n13700 (150, 150, 3)\n13800 (150, 150, 3)\n13900 (150, 150, 3)\n14000 (150, 150, 3)\n14100 (150, 150, 3)\n14200 (150, 150, 3)\n14300 (150, 150, 3)\n14400 (150, 150, 3)\n14500 (150, 150, 3)\n14600 (150, 150, 3)\n14700 (150, 150, 3)\n14800 (150, 150, 3)\n14900 (150, 150, 3)\n15000 (150, 150, 3)\n15100 (150, 150, 3)\n15200 (150, 150, 3)\n15300 (150, 150, 3)\n15400 (150, 150, 3)\n15500 (150, 150, 3)\n15600 (150, 150, 3)\n15700 (150, 150, 3)\n15800 (150, 150, 3)\n15900 (150, 150, 3)\n16000 (150, 150, 3)\n16100 (150, 150, 3)\n16200 (150, 150, 3)\n16300 (150, 150, 3)\n16400 (150, 150, 3)\n16500 (150, 150, 3)\n16600 (150, 150, 3)\n16700 (150, 150, 3)\n16800 (150, 150, 3)\n16900 (150, 150, 3)\n17000 (150, 150, 3)\n17100 (150, 150, 3)\n17200 (150, 150, 3)\n17300 (150, 150, 3)\n17400 (150, 150, 3)\n17500 (150, 150, 3)\n17600 (150, 150, 3)\n17700 (150, 150, 3)\n17800 (150, 150, 3)\n17900 (150, 150, 3)\n18000 (150, 150, 3)\n18100 (150, 150, 3)\n18200 (150, 150, 3)\n18300 (150, 150, 3)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.unique()"
      ],
      "metadata": {
        "id": "H8gHbuNHUq-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input\n",
        ")\n",
        "\n",
        "# Create ResNet50-based model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dense(650, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(300, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(124, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "num_classes = len(set(y_train))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Learning rate scheduling\n",
        "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Model checkpointing\n",
        "checkpoint_filepath = 'best_model.h5'\n",
        "model_checkpoint = callbacks.ModelCheckpoint(\n",
        "    checkpoint_filepath,\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model with data augmentation\n",
        "datagen.fit(X_train)  # Fit the ImageDataGenerator on your training data (only for normalization purposes)\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=32),\n",
        "    epochs=30,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[lr_scheduler, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Load the best model\n",
        "model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T13:39:29.492852Z",
          "iopub.execute_input": "2024-01-26T13:39:29.493363Z",
          "iopub.status.idle": "2024-01-26T14:28:22.345424Z",
          "shell.execute_reply.started": "2024-01-26T13:39:29.493323Z",
          "shell.execute_reply": "2024-01-26T14:28:22.344455Z"
        },
        "trusted": true,
        "id": "rjB73FVsUq-Z",
        "outputId": "4bee2fd8-661c-4ee9-9614-eb34560a75b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/30\n489/489 [==============================] - ETA: 0s - loss: 1.9138 - accuracy: 0.3496\nEpoch 1: val_loss improved from inf to 1.44485, saving model to best_model.h5\n489/489 [==============================] - 136s 203ms/step - loss: 1.9138 - accuracy: 0.3496 - val_loss: 1.4448 - val_accuracy: 0.5154 - lr: 1.0000e-04\nEpoch 2/30\n489/489 [==============================] - ETA: 0s - loss: 1.3278 - accuracy: 0.5144\nEpoch 2: val_loss did not improve from 1.44485\n489/489 [==============================] - 97s 197ms/step - loss: 1.3278 - accuracy: 0.5144 - val_loss: 1.5181 - val_accuracy: 0.4897 - lr: 1.0000e-04\nEpoch 3/30\n489/489 [==============================] - ETA: 0s - loss: 1.1298 - accuracy: 0.5667\nEpoch 3: val_loss improved from 1.44485 to 1.03465, saving model to best_model.h5\n489/489 [==============================] - 99s 203ms/step - loss: 1.1298 - accuracy: 0.5667 - val_loss: 1.0346 - val_accuracy: 0.5821 - lr: 1.0000e-04\nEpoch 4/30\n489/489 [==============================] - ETA: 0s - loss: 1.0464 - accuracy: 0.5933\nEpoch 4: val_loss did not improve from 1.03465\n489/489 [==============================] - 96s 197ms/step - loss: 1.0464 - accuracy: 0.5933 - val_loss: 1.2942 - val_accuracy: 0.5455 - lr: 1.0000e-04\nEpoch 5/30\n489/489 [==============================] - ETA: 0s - loss: 0.9927 - accuracy: 0.6118\nEpoch 5: val_loss did not improve from 1.03465\n489/489 [==============================] - 95s 195ms/step - loss: 0.9927 - accuracy: 0.6118 - val_loss: 1.0674 - val_accuracy: 0.5839 - lr: 1.0000e-04\nEpoch 6/30\n489/489 [==============================] - ETA: 0s - loss: 0.9607 - accuracy: 0.6253\nEpoch 6: val_loss did not improve from 1.03465\n489/489 [==============================] - 95s 195ms/step - loss: 0.9607 - accuracy: 0.6253 - val_loss: 1.0656 - val_accuracy: 0.5738 - lr: 1.0000e-04\nEpoch 7/30\n489/489 [==============================] - ETA: 0s - loss: 0.8647 - accuracy: 0.6542\nEpoch 7: val_loss improved from 1.03465 to 0.97468, saving model to best_model.h5\n489/489 [==============================] - 96s 197ms/step - loss: 0.8647 - accuracy: 0.6542 - val_loss: 0.9747 - val_accuracy: 0.6165 - lr: 1.0000e-05\nEpoch 8/30\n489/489 [==============================] - ETA: 0s - loss: 0.8184 - accuracy: 0.6769\nEpoch 8: val_loss improved from 0.97468 to 0.93457, saving model to best_model.h5\n489/489 [==============================] - 96s 196ms/step - loss: 0.8184 - accuracy: 0.6769 - val_loss: 0.9346 - val_accuracy: 0.6281 - lr: 1.0000e-05\nEpoch 9/30\n489/489 [==============================] - ETA: 0s - loss: 0.7827 - accuracy: 0.6900\nEpoch 9: val_loss did not improve from 0.93457\n489/489 [==============================] - 96s 196ms/step - loss: 0.7827 - accuracy: 0.6900 - val_loss: 0.9405 - val_accuracy: 0.6296 - lr: 1.0000e-05\nEpoch 10/30\n489/489 [==============================] - ETA: 0s - loss: 0.7760 - accuracy: 0.6930\nEpoch 10: val_loss improved from 0.93457 to 0.89741, saving model to best_model.h5\n489/489 [==============================] - 96s 197ms/step - loss: 0.7760 - accuracy: 0.6930 - val_loss: 0.8974 - val_accuracy: 0.6491 - lr: 1.0000e-05\nEpoch 11/30\n489/489 [==============================] - ETA: 0s - loss: 0.7610 - accuracy: 0.6953\nEpoch 11: val_loss did not improve from 0.89741\n489/489 [==============================] - 95s 194ms/step - loss: 0.7610 - accuracy: 0.6953 - val_loss: 0.9461 - val_accuracy: 0.6328 - lr: 1.0000e-05\nEpoch 12/30\n489/489 [==============================] - ETA: 0s - loss: 0.7453 - accuracy: 0.7057\nEpoch 12: val_loss did not improve from 0.89741\n489/489 [==============================] - 95s 194ms/step - loss: 0.7453 - accuracy: 0.7057 - val_loss: 0.9724 - val_accuracy: 0.6252 - lr: 1.0000e-05\nEpoch 13/30\n489/489 [==============================] - ETA: 0s - loss: 0.7198 - accuracy: 0.7108\nEpoch 13: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 196ms/step - loss: 0.7198 - accuracy: 0.7108 - val_loss: 0.9613 - val_accuracy: 0.6299 - lr: 1.0000e-05\nEpoch 14/30\n489/489 [==============================] - ETA: 0s - loss: 0.7034 - accuracy: 0.7190\nEpoch 14: val_loss did not improve from 0.89741\n489/489 [==============================] - 95s 195ms/step - loss: 0.7034 - accuracy: 0.7190 - val_loss: 0.9370 - val_accuracy: 0.6357 - lr: 1.0000e-06\nEpoch 15/30\n489/489 [==============================] - ETA: 0s - loss: 0.6979 - accuracy: 0.7286\nEpoch 15: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 196ms/step - loss: 0.6979 - accuracy: 0.7286 - val_loss: 0.9488 - val_accuracy: 0.6350 - lr: 1.0000e-06\nEpoch 16/30\n489/489 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.7305\nEpoch 16: val_loss did not improve from 0.89741\n489/489 [==============================] - 95s 195ms/step - loss: 0.6886 - accuracy: 0.7305 - val_loss: 0.9378 - val_accuracy: 0.6383 - lr: 1.0000e-06\nEpoch 17/30\n489/489 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.7290\nEpoch 17: val_loss did not improve from 0.89741\n489/489 [==============================] - 95s 195ms/step - loss: 0.6827 - accuracy: 0.7290 - val_loss: 0.9336 - val_accuracy: 0.6419 - lr: 1.0000e-06\nEpoch 18/30\n489/489 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.7286\nEpoch 18: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 195ms/step - loss: 0.6875 - accuracy: 0.7286 - val_loss: 0.9314 - val_accuracy: 0.6397 - lr: 1.0000e-06\nEpoch 19/30\n489/489 [==============================] - ETA: 0s - loss: 0.6871 - accuracy: 0.7272\nEpoch 19: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 196ms/step - loss: 0.6871 - accuracy: 0.7272 - val_loss: 0.9385 - val_accuracy: 0.6361 - lr: 1.0000e-06\nEpoch 20/30\n489/489 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.7279\nEpoch 20: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 196ms/step - loss: 0.6880 - accuracy: 0.7279 - val_loss: 0.9330 - val_accuracy: 0.6361 - lr: 1.0000e-06\nEpoch 21/30\n489/489 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.7313\nEpoch 21: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 196ms/step - loss: 0.6827 - accuracy: 0.7313 - val_loss: 0.9405 - val_accuracy: 0.6343 - lr: 1.0000e-06\nEpoch 22/30\n489/489 [==============================] - ETA: 0s - loss: 0.6728 - accuracy: 0.7352\nEpoch 22: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 197ms/step - loss: 0.6728 - accuracy: 0.7352 - val_loss: 0.9296 - val_accuracy: 0.6433 - lr: 1.0000e-06\nEpoch 23/30\n489/489 [==============================] - ETA: 0s - loss: 0.6720 - accuracy: 0.7362\nEpoch 23: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 197ms/step - loss: 0.6720 - accuracy: 0.7362 - val_loss: 0.9430 - val_accuracy: 0.6386 - lr: 1.0000e-06\nEpoch 24/30\n489/489 [==============================] - ETA: 0s - loss: 0.6722 - accuracy: 0.7336\nEpoch 24: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 196ms/step - loss: 0.6722 - accuracy: 0.7336 - val_loss: 0.9312 - val_accuracy: 0.6433 - lr: 1.0000e-06\nEpoch 25/30\n489/489 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.7306\nEpoch 25: val_loss did not improve from 0.89741\n489/489 [==============================] - 97s 198ms/step - loss: 0.6785 - accuracy: 0.7306 - val_loss: 0.9370 - val_accuracy: 0.6394 - lr: 1.0000e-06\nEpoch 26/30\n489/489 [==============================] - ETA: 0s - loss: 0.6815 - accuracy: 0.7318\nEpoch 26: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 197ms/step - loss: 0.6815 - accuracy: 0.7318 - val_loss: 0.9328 - val_accuracy: 0.6448 - lr: 1.0000e-06\nEpoch 27/30\n489/489 [==============================] - ETA: 0s - loss: 0.6745 - accuracy: 0.7337\nEpoch 27: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 195ms/step - loss: 0.6745 - accuracy: 0.7337 - val_loss: 0.9326 - val_accuracy: 0.6462 - lr: 1.0000e-06\nEpoch 28/30\n489/489 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.7301\nEpoch 28: val_loss did not improve from 0.89741\n489/489 [==============================] - 95s 194ms/step - loss: 0.6764 - accuracy: 0.7301 - val_loss: 0.9353 - val_accuracy: 0.6448 - lr: 1.0000e-06\nEpoch 29/30\n489/489 [==============================] - ETA: 0s - loss: 0.6718 - accuracy: 0.7387\nEpoch 29: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 196ms/step - loss: 0.6718 - accuracy: 0.7387 - val_loss: 0.9295 - val_accuracy: 0.6466 - lr: 1.0000e-06\nEpoch 30/30\n489/489 [==============================] - ETA: 0s - loss: 0.6613 - accuracy: 0.7402\nEpoch 30: val_loss did not improve from 0.89741\n489/489 [==============================] - 96s 197ms/step - loss: 0.6613 - accuracy: 0.7402 - val_loss: 0.9369 - val_accuracy: 0.6419 - lr: 1.0000e-06\n87/87 [==============================] - 3s 28ms/step\nAccuracy: 0.649148242116709\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abs_train_path = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/train.csv'\n",
        "abs_train_directory = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/train'\n",
        "abs_test_path = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/sample_submission.csv'\n",
        "abs_test_directory = '/kaggle/input/extracting-attributes-from-fashion-images-jan-2024/test'\n",
        "\n",
        "df2 = pd.read_csv(abs_test_path)\n",
        "\n",
        "X_test = []\n",
        "\n",
        "# Resize and convert to grayscale for the test set\n",
        "for i in range(df2.shape[0]):\n",
        "        img_path = os.path.join(abs_test_directory, df2.iloc[i]['file_name'])\n",
        "        label = df2.iloc[i]['label']\n",
        "\n",
        "        # Read the image using cv2.imread\n",
        "        img = cv2.imread(img_path)\n",
        "        # Resize the image\n",
        "        img_resized = cv2.resize(img, (150, 150))\n",
        "        # Perform wavelet transform\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(i, img_resized.shape)\n",
        "        X_test.append(img_resized)\n",
        "\n",
        "\n",
        "X_test_np = np.array(X_test)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T14:28:57.762281Z",
          "iopub.execute_input": "2024-01-26T14:28:57.763267Z",
          "iopub.status.idle": "2024-01-26T14:29:51.766482Z",
          "shell.execute_reply.started": "2024-01-26T14:28:57.763226Z",
          "shell.execute_reply": "2024-01-26T14:29:51.765420Z"
        },
        "trusted": true,
        "id": "0DJ1kFD5Uq-a",
        "outputId": "b4b47713-47ee-4790-c80d-92a3c33683e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "0 (150, 150, 3)\n100 (150, 150, 3)\n200 (150, 150, 3)\n300 (150, 150, 3)\n400 (150, 150, 3)\n500 (150, 150, 3)\n600 (150, 150, 3)\n700 (150, 150, 3)\n800 (150, 150, 3)\n900 (150, 150, 3)\n1000 (150, 150, 3)\n1100 (150, 150, 3)\n1200 (150, 150, 3)\n1300 (150, 150, 3)\n1400 (150, 150, 3)\n1500 (150, 150, 3)\n1600 (150, 150, 3)\n1700 (150, 150, 3)\n1800 (150, 150, 3)\n1900 (150, 150, 3)\n2000 (150, 150, 3)\n2100 (150, 150, 3)\n2200 (150, 150, 3)\n2300 (150, 150, 3)\n2400 (150, 150, 3)\n2500 (150, 150, 3)\n2600 (150, 150, 3)\n2700 (150, 150, 3)\n2800 (150, 150, 3)\n2900 (150, 150, 3)\n3000 (150, 150, 3)\n3100 (150, 150, 3)\n3200 (150, 150, 3)\n3300 (150, 150, 3)\n3400 (150, 150, 3)\n3500 (150, 150, 3)\n3600 (150, 150, 3)\n3700 (150, 150, 3)\n3800 (150, 150, 3)\n3900 (150, 150, 3)\n4000 (150, 150, 3)\n4100 (150, 150, 3)\n4200 (150, 150, 3)\n4300 (150, 150, 3)\n4400 (150, 150, 3)\n4500 (150, 150, 3)\n4600 (150, 150, 3)\n4700 (150, 150, 3)\n4800 (150, 150, 3)\n4900 (150, 150, 3)\n5000 (150, 150, 3)\n5100 (150, 150, 3)\n5200 (150, 150, 3)\n5300 (150, 150, 3)\n5400 (150, 150, 3)\n5500 (150, 150, 3)\n5600 (150, 150, 3)\n5700 (150, 150, 3)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test_np)\n",
        "predicted_labels = tf.argmax(predictions, axis=1).numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T14:30:15.402801Z",
          "iopub.execute_input": "2024-01-26T14:30:15.403442Z",
          "iopub.status.idle": "2024-01-26T14:30:21.869325Z",
          "shell.execute_reply.started": "2024-01-26T14:30:15.403408Z",
          "shell.execute_reply": "2024-01-26T14:30:21.868472Z"
        },
        "trusted": true,
        "id": "My95eQgvUq-a",
        "outputId": "90bd804c-f8f3-43cc-b173-9a3452b28b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "180/180 [==============================] - 5s 30ms/step\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with the file_name and label columns\n",
        "df_output = pd.DataFrame({'file_name': df2[\"file_name\"], 'label': predicted_labels})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_output.to_csv('output9.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T14:30:25.169798Z",
          "iopub.execute_input": "2024-01-26T14:30:25.170672Z",
          "iopub.status.idle": "2024-01-26T14:30:25.194257Z",
          "shell.execute_reply.started": "2024-01-26T14:30:25.170641Z",
          "shell.execute_reply": "2024-01-26T14:30:25.193462Z"
        },
        "trusted": true,
        "id": "m-XiGw-4Uq-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}